---
강의 리스트:
  - "[[기계학습]]"
생성 일시: 2024-04-17
유형: 시험
주차:
  - 중간
---
### 기계학습

- 지도학습
    - 분류 : 데이터가 어떤 클래스에 속할지 예측
        - Binary Classification : 클래스 두개
        - Multiclass Classification : 클래스가 두개 이상 중 하나
        - Multi-label Classification : 동시에 여러개의 클래스에 속함
    - 회귀 : 데이터를 잘 나누는 선을 찾아 결과값 예측
- 비지도학습
    - 군집화
    - 차원축소
- 강화학습

### ==학습의 과정==

==svm - feature scaling==

- 특성 값들을 비슷한 범위로 매핑해주는 것 Ex) 정규화, 표준화
- 표준화 : 각 값에서 평균을 빼고 표준편차로 나눈 값을 사용
- 정규화 : 최소-최대 정규화, 최솟값을 0 최댓값을 1로 변환

==decision tree - feature scaling 하지 않음==

### ==회귀, 분류의 정확도 측정 방법 차이==

- 회귀
    - MSE(Mean Squared Error) → 에러(실제값 - 예측값) 제곱의 평균
    - RMSE(Root MSE) → MSE에 루트
    - R2 Score → 실제값의 분산 대비 예측값의 분산 비율
- 분류
    - Accuracy : 실제 데이터와 예측 데이터가 얼마나 같은지 판단하는 지표
        
        → 데이터의 일치 여부를 판단하는 거기 때문에 회귀에는 사용 X
        

### ==naive bays - 2문제 정도==

==베이즈 포뮬라!!==

![[/Untitled 28.png|Untitled 28.png]]

![[/Untitled 1 7.png|Untitled 1 7.png]]

- Prior : 사전확률, 우리가 다루는 데이터들과는 전혀 관련이 없는 확률
    
    ex) 연어가 물에서 잡힐 확률 - 50%
    
- Likelihood : 우리가 다루는 데이터로부터 도출해낸 확률 분포
- Evidence : 클래스와는 관련이 없는 확률값
- Posterior : 어떤 클래스에 해당할지 예측한 확률

  

### Naive Bayes

Naive : 예측하려는 특징이 상호 독립적이라는 가정 하에 확률 계산을 단순화 함

- 실습 문제
    
    ![[/Untitled 2 6.png|Untitled 2 6.png]]
    
    ![[/Untitled 3 4.png|Untitled 3 4.png]]
    
    ![[/Untitled 4 4.png|Untitled 4 4.png]]
    
      
    

  

### ==svm - 3문제 정도==

### Support Vector Machine

- 관측값을 가장 잘 분리할 수 있는 최적의 초평면(하이퍼플레인) 찾는게 목표
- 최적의 초평면은 이 초평면과 각 공간의 가장 가까운 점과의 거리가 최대가 되도록 하는 것  
    이때 이 점을 Support Vector 라고 함  
    
- 모든 데이터 포인트는 다음 식을 만족하고, 클래스 별로 공간이 나눠지면 분리 초평면이라고 함
    
    SVM의 목적은 이때의 w와 b를 찾는 것!
    

$$w^Tx +b=0$$

==분류 회귀에 전부 쓰임==

==마진 : 초평면과 각 클래스에서 가장 가까운 관측치 사이의 거리==

![[/Untitled 5 3.png|Untitled 5 3.png]]

==목적함수-선형 비선형==

==커널기법-rbf==

![[/Untitled 6 3.png|Untitled 6 3.png]]

### ==SVR==

==목적함수!!==

### ==DEcision tree==

==용어 정의==

- 트리처럼 생긴 그래프로, 가능한 모든 의사결정의 대안과 그에 대한 결과를 보여주는 다이어그램
- 루트 - 노드 - 리프(단말노드), 분기(branch)

==회귀, 분류의 차이==

- 회귀 : 비슷한 수치를 갖는 관측치끼리 묶음
- 분류 : 같은 범주를 갖는 관측치끼리 묶음

==비용함수 지표 세가지!!==

1. 오분류율 : 실제 범주와 모델이 예측한 범주가 얼마나 일치하는지
2. 지니 불순도 : 클래스 분포의 불순도 비율(혼합 비율) 측정
3. 정보이득 : 분할 이후 불확실성 감소를 측정, 높을수록 더 나은 분할을 의미

==장단점==

- 장점
    - 범주형과 수치형 데이터 모두 예측 가능
    - 구조 단순, 해석 용이
    - 수학적 가정이 불필요한 비모수적 모형
- 단점
    - 경계선 근방의 자료에 대해서는 오차가 클 수 있음
    - 트리구조 형성단계에서 에러가 발생하면 에러가 전파됨
    - 노이즈에 민감해서 미세한 변동에도 영향이 큼
    - 데이터 수가 적을때 불안정함

### ==regression==

==linear==

- 독립변수(x)와 종속변수(y) 사이의 관계를 선형 결합으로 표현
- ==가정이 필요하다!!==
    1. 예측값과 상관없이 오차의 모든 분산이 동일함
    2. 오차항은 서로 독립
    3. 오차항은 정규 분포 형태
- ==비용함수, 최적의 함수 어케 구하는지==

==logistic==

- 종속변수가 binary같은 범주형 변수 → 주로 분류에 이용됨
- ==odds 개념==
    - ==샘플 x가 양성값일 확률을 p라고 정의할때 1-p는 음성 값일 가능성을 나타냄,== 이때 $\frac{p}{1-p}$
- ==logit transform!!==
    - $log(Odds) = \beta_0 + \beta_1x$
- ==likelihood==
    - 관측치가 올바른 클래스로 분류될 것으로 예상되는확률
    - 보통 log likelyhood 사용 → $Log(L)$
    - $Max Log(L) = Min -Log(L)$ → Cross Entropy 최소화

### ==실습에서만 배웠던 내용들==

==bow의 의미==

- ==단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법==

==불용어==

- 자연어 처리에서 큰 의미를 갖지 않는 단어들
- 직접정의 `CountVectorizer(stop_words=[”asdf”, “asdf”])`

==DTM이 뭔지-한계==

- DTM : 문서 단어 행렬 (Document-Term Matrix) : 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것
- DTM의 한계
    1. 희소 표현 - 원핫벡터나 DTM과 같은 대부분의 값이 0인 벡터, 공간 낭비, 계산 복잡도 증가, 전처리로 단어 집합의 크기 축소
    2. 단순 빈도 수 기반 접근 - 불용어 처럼 빈도는 많지만 의미가 없는 단어 존재 → 중요한 단어에 가중치를 준 TF-IDF 등장

==TF-IDF이 뭔지-의미==

- DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 가중치
- 문서 유사도 구하기, 검색 중요도 정하기 등에 사용
- TF 와 IDF를 곱한 값을 의미
    - TF : 문서에서 특정 단어의 등장 횟수
    - DF : 특정 단어가 등장한 문서의 수
    - IDF : DF의 역수, 분모가 0이 되는걸 막기 위해 1 더해줌, 값이 기하급수적으로 커지는 걸 막기 위해 log 취해줌
- TF-IDF 값이 낮으면 중요도 낮, 크다면 중요도 높