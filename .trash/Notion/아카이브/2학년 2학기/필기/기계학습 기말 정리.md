---
강의 리스트:
  - "[[기계학습]]"
생성 일시: 2024-06-12
유형: 시험
주차:
  - 기말
---
### 시험 대비

week9

- 모델 평가
    - 에큐러시 - 올바르게 예측한 비율 (1-오분류율)
        - 하지만 false positive와 false negative를 모두 같은 비율로 평가함
        - 클래스 표본이 불균형한 경우 정확한 평가 불가
    - 컨퓨전 매트릭스
        - Sensitivity - 참을 참이라고 예측한 비율
        - Specificity - 거짓을 거짓이라고 예측한 비율
        - Precision - 참이라고 예측한것중 실제로 참인 비율
    - ROC 커브, AUC
    - 해밍 스코어

  

week10

- KNN
    - 모델학습없이 거리로 유사한것 예측
    - 장점
        - 알고리즘 간단해서 구현 쉬움
        - 적은 하이퍼 파라미터
        - 데이터 수 많을 때 효과적
    - 단점
        - 데이터간 거리 측정이 필요해서 계산 비용이 큼, 시간 오래걸림
    - 분류문제에서는 다수결로 계산, 예측은 평균값에 의해 계산된다!
    - 하이퍼 파라미터
    - k어케 결정하는지, k에 따른 변화점
        - k 작음 → 오버피팅, 지역적 특성 과 반영
        - k 높음 → 언더피팅
    - 데이터간 거리 측정법 4가지 - 이때 피쳐 스케일링 중요!
        - 유클리디안 거리 - 두 관측치 사이의 직선거리 (L2 distance)
        - 맨해튼 거리 - 격자거리 (L1 distance)
        - mahalanobis 거리 - 타원방정식
        - Correlation 거리 - 귀찮… 0부터 2 사이의 값 가짐
- weighted knn 장단점
    - 단순 다수결의 문제점을 해결,

  

week11

- 앙상블 모델
    - 왜 앙상블 하는가 - 다양성 확보 위해 ↔ KNN은 하나의 fold 손해봄
    - 배깅 이라는 개념, 왜 좋은가  
        랜덤 복원 추출 데이터 샘플링, 각 샘플은 부트스트랩, 추출 되지 않은 데이터셋은 OOB라고 부르며 검증을 위해 사용  
        - 메이져리티 보팅
        - 웨이티드 보팅
        - 스태킹 - 개별 모델들의 결과값을 입력으로 받는 메타러너 추가됨
    - 랜덤 포레스트 - 배깅으로 만든 의사결정 나무, 변수를 랜덤하게 설정, 각 부트스트랩마다 의사결정 나무가 각각 학습
        - 예시 중요
        - 성능 높이려면 무얼 해야하는지
            - 개별 트리의 정확도, 독립성 높을수록 랜포 성능 업
        - 변수 중요도, 계산 스탭
        - 하이퍼 파라미터 종류
            - 트리의 수, 노드 분할시 선택되는 변수의 수(분류는 변수의 수의 제곱근, 회귀는 변수의 수 / 트리 개수)
        - 장점
            - 출력변수와 입력변수간 복잡한 관계 모델링 가능
            - 예측력 좋음
            - 이상치에 강함
        - 단점
            - 모형 해석이 어려움
            - 학습 시간이 김
            - 모델의 선택권이 넓은 배깅과 달리 의사결정트리로 정해져있음
    - 부스팅
        
        - 원리 - weak learner들을 연속으로 앙상블, 이전 단계 러너의 오차를 반영해줌
        - 아다부스트 - 이전 단계 러너의 오차를 가중치로 주는 방식
        - 배깅과 부스팅의 차이 - 각 앙상블이 영향을 미치지 않는 배깅과 달리 부스팅은 가중치로서 서로 영향이 생김
        - GBM의 원리 - gradient descent로 로스펑션이 줄어드는 방향으로 weak learners를 반복 결합시키는 알고리즘  
            노이즈까지 학습하기 때문에 오버피팅 위험이 있음  
            
        
          
        

week12 - 뉴럴 네트워크

- 퍼셉트론 - 구성, 히든노드의 두 역할(앞단, 뒷단)
    - 활성화 함수 - 종류(인풋, 아웃풋 함수), 특징
        - 시그모이드 - 0~1 사이의 값, 가장 많이 사용되나 기울기 소실 문제 존재
        - 하이퍼볼릭 탄젠트 - (-1~1) 사이의 값, 시그모이드보다 기울기 소실 문제 적음
        - ReLU - 히든레이어에서 가장 많이 쓰임, 음수 입력시 0 출력, 양수는 그대로, 연산속도 빠름, dying relu 문제 존재
    - 로스 펑션- 출력과 실제 값의 차이
    - 그레디언트 디센트 (별로 안중요?)
    - 웨잇 업데이트 주기에 따른 세가지 방법
        - SGD - 각 학습마다 로스펑션 계산하고 업데이트
        - BGD - 학습이 끝나고 한번에 업데이트
        - 미니배치GD - 미니 배치마다 업데이트
    - 러닝 레잇에 따른 차이점
        - 작으면 → 수렴까지 오랜시간이 걸림
        - 크면 → 수렴되지 않을 위험성 존재
    - 옵티마이저 종류

  

week13

- 퍼셉트론의 한계 → 예측변수와 결과의 관계가 선형이 아니면 예측 성능이 떨어짐
    - 분류 : 선형으로만 바운더리 찾을 수 있음
    - 회귀 : 선형관계만 살펴 볼 수 있음
- MLP
    - 뭔지, 어케 한계를 넘었는지
        - 이진분류는 시그모이드, 멀티 클래스 분류에는 소프트 맥스 사용
    - 에러 백 프로파게이션(역전파) - 계산 문제는 안나올 예정

  

week14 - 텍스트 분석

- 프리 프로세싱
    - 텍스트 분석이 뭔지, 전체 과정
    - 전처리 과정 (영어, 한국어) - 별도 자료(워드파일)도 참고
        - 둘의 차이점 기억!
            - 한글 - 하나의 단어가 하나 이상의 형태소의 조합으로 구성
            - 영어 - 하나의 단어가 하나의 형태소로 구성, 그 형태도 변형 가능
        - 영어
            1. 기호제거
            2. 대소문자 변경
            3. 토큰화 (단어 단위로 짜르기)
            4. 품사 태깅
            5. Lemmatization - 어근 찾기
            6. 불용어 제거
        - 한국어
            1. 기호제거
            2. 형태소 분석 (토큰화 + 품사태깅 + Lemmatizing)
            3. 필요한 품사의 단어들만 선택
            4. 불용어 제거

  

week15 - 감성 분석

- 감성분석
    - 두가지 접근법 - 렉시콘(감성어 사전 기반)과 머신러닝
- 워드 임베딩 - 단어를 벡터로 표현하는 것
    - 두가지 방법 (원핫 인코딩/ 워드 2 벡)
    - 디스트리뷰티드 리프레센테이션
    - 워드 2백의 두가지 방법 (스킵 그램/ 어쩌구)
- 토픽 모델링
    - LDA - 식 공부 X

  

코드실습

- 뉴럴 넷 부분 많이 나옴

  

  

---

MLP keras 파라미터 갯수

  

사진 한장 = 28 * 28 = 784

첫번째 레이어 노드 수 = 512

바이어스 = 512

  

784 * 512 + 512 = 401,920 개

  

week13 실습자료 많이 참고